"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[506],{2196:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_2-24ad92a3612a1c18937a60ddde03f385.png"},3325:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_3-3e33d8f8e3007b5184d9e0ba1badb7df.png"},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>m});var r=t(6540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){a(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},g=r.forwardRef(function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),g=a,m=u["".concat(s,".").concat(g)]||u[g]||d[g]||i;return t?r.createElement(m,o(o({ref:n},p),{},{components:t})):r.createElement(m,o({ref:n},p))});function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},7311:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_1-1b28c489a3d9d8cc9fef5dc2aab7ead7.png"},8572:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var r=t(8168),a=(t(6540),t(5680));const i={},o="Quickstart: Singel Node based on Alibaba Cloud",l={unversionedId:"English/QuickStart/alicloud_pipeline_quick_start_en",id:"English/QuickStart/alicloud_pipeline_quick_start_en",title:"Quickstart: Singel Node based on Alibaba Cloud",description:"Environment Preparation",source:"@site/docs/English/QuickStart/alicloud_pipeline_quick_start_en.md",sourceDirName:"English/QuickStart",slug:"/English/QuickStart/alicloud_pipeline_quick_start_en",permalink:"/ROLL/docs/English/QuickStart/alicloud_pipeline_quick_start_en",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/English/QuickStart/alicloud_pipeline_quick_start_en.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",next:{title:"Configuration Guide",permalink:"/ROLL/docs/English/QuickStart/config_guide"}},s={},c=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"Pipeline Execution",id:"pipeline-execution",level:2}],p={toc:c},u="wrapper";function d({components:e,...n}){return(0,a.yg)(u,(0,r.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"quickstart-singel-node-based-on-alibaba-cloud"},"Quickstart: Singel Node based on Alibaba Cloud"),(0,a.yg)("h2",{id:"environment-preparation"},"Environment Preparation"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Purchase an Alibaba Cloud Server")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"For a single-machine setup, consider a GPU instance with ",(0,a.yg)("strong",{parentName:"li"},"NVIDIA V100"),"."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Recommendation:")," When purchasing a GPU instance via the ECS console, it's advised to select the option to automatically install GPU drivers.")),(0,a.yg)("ol",{start:2},(0,a.yg)("li",{parentName:"ol"},"Remote Connect to the GPU Instance and access the machine terminal"),(0,a.yg)("li",{parentName:"ol"},"Install NVIDIA Container Toolkit")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \\\n  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\n  \nsudo yum install -y nvidia-container-toolkit\n")),(0,a.yg)("ol",{start:4},(0,a.yg)("li",{parentName:"ol"},"Install Docker Environment\uff1arefer to ",(0,a.yg)("a",{parentName:"li",href:"https://developer.aliyun.com/mirror/docker-ce/"},"https://developer.aliyun.com/mirror/docker-ce/"))),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"# step 1: install necessary system tools\nsudo yum install -y yum-utils\n\n# Step 2: add software repository information\nsudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n# Step 3: install Docker\nsudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n\n# Step 4: start Docker service\nsudo service docker start\n\n# Verify that GPUs are visible\ndocker version\n")),(0,a.yg)("h2",{id:"environment-configuration"},"Environment Configuration"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"# 1. Pull Docker image\nsudo docker pull <image_address>\n\n# Image Addresses (choose based on your needs)\n# torch2.6.0 + SGlang0.4.6: roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-sglang046\n# torch2.6.0 + vLLM0.8.4: roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-vllm084\n# torch2.5.1 + SGlang0.4.3: roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch251-sglang043\n# torch2.5.1 + vLLM0.7.3: roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch251-vllm073\n\n# 2. Start a Docker container with GPU support and keep the container running\nsudo docker images\nsudo docker run -dit \\\n  --gpus all \\\n  --network=host \\\n  --ipc=host \\\n  --shm-size=2gb \\\n  <image_id> \\\n  /bin/bash\n\n# 3. Enter the Docker container (execute this command every time you reconnect)\nsudo docker ps\nsudo docker exec -it <container_id> /bin/bash\n\n# 4. Verify GPU visibility\nnvidia-smi\n\n# 5. Download the code\n\n# Install git\uff08this image is ubuntu-based\uff09and clone the repo\napt update && apt install git -y\ngit clone https://github.com/alibaba/ROLL.git\n\n# If Github is not accessible, download the zip file and unzip it\nwget https://github.com/alibaba/ROLL/archive/refs/heads/main.zip\nunzip main.zip\n\n# 5. Install dependencies (select the requirements file corresponding to your chosen image)\ncd ROLL-main\npip install -r requirements_torch260_sglang.txt -i https://mirrors.aliyun.com/pypi/simple/\n")),(0,a.yg)("h2",{id:"pipeline-execution"},"Pipeline Execution"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},'# If you encounter "ModuleNotFoundError: No module named \'roll\'", you need to add environment variables\nexport PYTHONPATH="/workspace/ROLL-main:$PYTHONPATH"\n\n# Method 1: Specify the YAML file path, with the script directory (examples) as the root\npython examples/start_agentic_pipeline.py --config_path qwen2.5-0.5B-agentic_ds  --config_name agent_val_frozen_lake\n\n# Method 2: Execute the shell script directly\nbash examples/qwen2.5-0.5B-agentic_ds/run_agentic_pipeline_frozen_lake.sh\n\n# Modify the configuration as needed\nvim examples/qwen2.5-0.5B-agentic_ds/agent_val_frozen_lake.yaml\n')),(0,a.yg)("p",null,"Key Configuration Modifications for Single V100 GPU Memory:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"# Reduce the system's expected number of GPUs from 8 to your actual 1 V100\nnum_gpus_per_node: 1 \n# Training processes are now mapped only to GPU 0\nactor_train.device_mapping: list(range(0,1))\n# Inference processes are now mapped only to GPU 0\nactor_infer.device_mapping: list(range(0,1))\n# Reference model processes are now mapped only to GPU 0\nreference.device_mapping: list(range(0,1))\n\n# Significantly reduce the batch sizes for Rollout and Validation stages to prevent out-of-memory errors on a single GPU\nrollout_batch_size: 16\nval_batch_size: 16\n\n# V100 has better native support for FP16 than BF16 (unlike A100/H100). Switching to FP16 improves compatibility and stability, while also saving GPU memory.\nactor_train.model_args.dtype: fp16\nactor_infer.model_args.dtype: fp16\nreference.model_args.dtype: fp16\n\n# Switch the large model training framework from DeepSpeed to Megatron-LM. Parameters can be sent in batches, resulting in faster execution.\nstrategy_name: megatron_train\nstrategy_config:\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  expert_model_parallel_size: 1\n  use_distributed_optimizer: true\n  recompute_granularity: full\n\n# In megatron training the global train batch size is equivalent to per_device_train_batch_size * gradient_accumulation_steps * world_size\nactor_train.training_args.per_device_train_batch_size: 1\nactor_train.training_args.gradient_accumulation_steps: 16  \n\n# Reduce the maximum number of actions per trajectory, making each Rollout trajectory shorter that reduces the length of LLM-generated content.\nmax_actions_per_traj: 10    \n\n# Reduce the number of parallel training and validation environment groups to accommodate single-GPU resources.\ntrain_env_manager.env_groups: 1\ntrain_env_manager.n_groups: 1\nval_env_manager.env_groups: 2\nval_env_manager.n_groups: [1, 1]\nval_env_manager.tags: [SimpleSokoban, FrozenLake]\n\n# Reduce the total number of training steps for quicker full pipeline runs, useful for rapid debugging.\nmax_steps: 100\n")),(0,a.yg)("p",null,"Example Log Screenshots during Pipeline Execution:\n",(0,a.yg)("img",{alt:"log1",src:t(7311).A,width:"2868",height:"650"})),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"log2",src:t(2196).A,width:"2876",height:"904"})),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"log3",src:t(3325).A,width:"1904",height:"206"})))}d.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[703],{3144:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>y,frontMatter:()=>r,metadata:()=>o,toc:()=>g});var a=i(8168),t=(i(6540),i(5680));const r={},l="Comprehensive Guide: Using the Agentic Part of ROLL",o={unversionedId:"English/StepByStep/agent_pipeline_start",id:"English/StepByStep/agent_pipeline_start",title:"Comprehensive Guide: Using the Agentic Part of ROLL",description:"Table of Contents",source:"@site/docs/English/StepByStep/agent_pipeline_start.md",sourceDirName:"English/StepByStep",slug:"/English/StepByStep/agent_pipeline_start",permalink:"/ROLL/docs/English/StepByStep/agent_pipeline_start",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/English/StepByStep/agent_pipeline_start.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Installation",permalink:"/ROLL/docs/English/QuickStart/installation"},next:{title:"RLVR Pipeline",permalink:"/ROLL/docs/English/StepByStep/rlvr_pipeline_start"}},p={},g=[{value:"Overview",id:"overview",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Components",id:"core-components",level:2},{value:"Agentic Pipeline (<code>AgenticPipeline</code>)",id:"agentic-pipeline-agenticpipeline",level:3},{value:"Agentic Configuration (<code>AgenticConfig</code>)",id:"agentic-configuration-agenticconfig",level:3},{value:"Environments (<code>BaseEnv</code> and implementations)",id:"environments-baseenv-and-implementations",level:3},{value:"Models &amp; Workers",id:"models--workers",level:3},{value:"Running the Agentic Pipeline",id:"running-the-agentic-pipeline",level:2},{value:"Method\xa01: Using the Python Launcher Script with Hydra (Recommended)",id:"method1-using-the-python-launcher-script-with-hydra-recommended",level:3},{value:"Method\xa02: Using Helper Shell Scripts",id:"method2-using-helper-shell-scripts",level:3},{value:"Step-by-Step Example",id:"step-by-step-example",level:2},{value:"Step\xa01: Locate and Review Configuration",id:"step1-locate-and-review-configuration",level:3},{value:"Step\xa02: Prepare the Environment and Dependencies",id:"step2-prepare-the-environment-and-dependencies",level:3},{value:"Step\xa03: Launch the Pipeline",id:"step3-launch-the-pipeline",level:3},{value:"Step\xa04: Monitoring",id:"step4-monitoring",level:3},{value:"Step\xa05: Outputs and Results",id:"step5-outputs-and-results",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Further Information",id:"further-information",level:2}],s={toc:g},m="wrapper";function y({components:e,...n}){return(0,t.yg)(m,(0,a.A)({},s,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"comprehensive-guide-using-the-agentic-part-of-roll"},"Comprehensive Guide: Using the Agentic Part of ROLL"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Table of Contents")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#overview"},"Overview")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#introduction"},"Introduction")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#prerequisites"},"Prerequisites")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#core-components"},"Core Components")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#running-the-agentic-pipeline"},"Running the Agentic Pipeline")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-by-step-example"},"Step-by-Step Example")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#troubleshooting"},"Troubleshooting")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#further-information"},"Further Information"))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"overview"},"Overview"),(0,t.yg)("p",null,"The ROLL (Reinforcement Learning Optimization for Large-Scale Learning) agentic pipeline empowers you to:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Define, configure, and simulate interactions between AI agents (often Large Language Models) and diverse environments.  "),(0,t.yg)("li",{parentName:"ul"},"Train these agents using reinforcement learning algorithms like Proximal Policy Optimization (PPO), GRPO, and ",(0,t.yg)("strong",{parentName:"li"},"reinforce++"),".  "),(0,t.yg)("li",{parentName:"ul"},"Evaluate agent performance on specific tasks and complex reasoning scenarios.  "),(0,t.yg)("li",{parentName:"ul"},"Leverage ",(0,t.yg)("a",{parentName:"li",href:"https://www.ray.io/"},"Ray")," for efficient, distributed computation across large-scale GPU setups.")),(0,t.yg)("p",null,"This guide provides a step-by-step walkthrough for utilizing these agentic capabilities."),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"introduction"},"Introduction"),(0,t.yg)("p",null,"This tutorial guides you through setting up, configuring, and running the agentic components of the ROLL library. The agentic part of ROLL is specifically designed for building and training AI agents that learn through interaction. This is crucial for tasks like aligning LLMs with human preferences, teaching them complex reasoning, or improving their performance in multi-turn interaction scenarios."),(0,t.yg)("p",null,"We will cover the general workflow, which you can adapt for various environments and agent configurations based on your specific research or application needs."),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,t.yg)("p",null,"Before you begin, ensure you have the following:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"ROLL Project Installed")," \u2013 The ROLL project should be cloned and properly set up. Please refer to the main ",(0,t.yg)("inlineCode",{parentName:"p"},"README.md")," in the project root for detailed installation instructions. Key requirements often include:  "),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"CUDA Version >= 12.4  "),(0,t.yg)("li",{parentName:"ul"},"cuDNN Version >= 9.1.0  "),(0,t.yg)("li",{parentName:"ul"},"PyTorch >= 2.5.1  "),(0,t.yg)("li",{parentName:"ul"},"vLLM >= 0.7.3  "),(0,t.yg)("li",{parentName:"ul"},"SGlang >= 0.4.3",(0,t.yg)("br",{parentName:"li"}),"ROLL also provides Docker images for a quick start."))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Python Dependencies")," \u2013 Install all necessary Python dependencies, typically via the requirements file:"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install -r requirements.txt   # Or a specific file like requirements_torch260.txt\n")),(0,t.yg)("p",{parentName:"li"},"Ensure any specific dependencies for your chosen agentic environments or models are also met.")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Python Environment")," \u2013 A working Python environment (python=3.10, or use our docker_images).")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Ray Setup (Recommended for Distributed Execution)"),"  "),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Install Ray:"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install ray\n"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"The project may contain example scripts for starting a Ray cluster, such as ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/scripts/start_ray_cluster.sh"),".")))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Environment Variables")," \u2013 Some scripts might require specific environment variables (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"PYTHONPATH"),"). The provided execution scripts (like those in ",(0,t.yg)("inlineCode",{parentName:"p"},"examples"),") often handle setting these."))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"core-components"},"Core Components"),(0,t.yg)("h3",{id:"agentic-pipeline-agenticpipeline"},"Agentic Pipeline (",(0,t.yg)("inlineCode",{parentName:"h3"},"AgenticPipeline"),")"),(0,t.yg)("p",null,"The main orchestrator for the agentic RL process. It manages the entire workflow, including:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Initializing and managing distributed workers (actors, critics, environment managers).  "),(0,t.yg)("li",{parentName:"ul"},"Coordinating data collection (rollouts) where the agent interacts with the environment.  "),(0,t.yg)("li",{parentName:"ul"},"Executing model training steps (e.g., PPO updates for actor and critic).  "),(0,t.yg)("li",{parentName:"ul"},"Handling model synchronization and checkpointing.  "),(0,t.yg)("li",{parentName:"ul"},"Running evaluation loops.  "),(0,t.yg)("li",{parentName:"ul"},"Logging metrics and experiment tracking.")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Source Code"),": ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/agentic_pipeline.py")),(0,t.yg)("hr",null),(0,t.yg)("h3",{id:"agentic-configuration-agenticconfig"},"Agentic Configuration (",(0,t.yg)("inlineCode",{parentName:"h3"},"AgenticConfig"),")"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"AgenticConfig")," (defined in ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/agentic_config.py"),") is a Pydantic/dataclass-based configuration object that specifies all parameters for an agentic pipeline run. It is typically defined and populated from a YAML file using Hydra."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Role"),": Holds settings for environments, agent models (actor, critic, reference), reward mechanisms, training algorithms (e.g., PPO hyperparameters), rollout procedures, logging, and distributed execution strategies."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Nested Configurations")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"actor_train"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"actor_infer"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"critic"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"reference")," \u2013 Instances of ",(0,t.yg)("inlineCode",{parentName:"li"},"WorkerConfig")," defining the models, hardware, and strategies for each role.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"train_env_manager"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"val_env_manager")," \u2013 Instances of ",(0,t.yg)("inlineCode",{parentName:"li"},"EnvManagerConfig")," specifying how environments are instantiated and managed for training and validation rollouts.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_normalization")," \u2013 Configuration for normalizing rewards.")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Example Configurations")," \u2013 See example YAML files in directories like ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-0.5B-agentic_ds")," (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"agent_val_frozen_lake.yaml"),"). These YAMLs use Hydra to set the fields within ",(0,t.yg)("inlineCode",{parentName:"p"},"AgenticConfig"),"."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key parameters often set via YAML for ",(0,t.yg)("inlineCode",{parentName:"strong"},"AgenticConfig")," include")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"exp_name"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"seed"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"logging_dir"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"output_dir")," \u2013 General experiment settings.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_steps"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"save_steps"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"logging_steps"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"eval_steps")," \u2013 Pipeline control for duration and frequencies.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pretrain"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"reward_pretrain")," \u2013 Paths to base models for actor/reference and critic respectively.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"init_kl_coef"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"gamma"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"lambd")," \u2013 PPO and RL algorithm parameters.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"custom_envs")," \u2013 A dictionary defining custom environments available to the pipeline, including their type, instructions, and specific configurations.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"train_env_manager")," / ",(0,t.yg)("inlineCode",{parentName:"li"},"val_env_manager")," settings:  ",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"env_groups"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"group_size")," \u2013 For managing parallel environments.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"tags"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"n_groups")," \u2013 To select and apportion environments from ",(0,t.yg)("inlineCode",{parentName:"li"},"custom_envs"),".  "))),(0,t.yg)("li",{parentName:"ul"},"Worker-specific settings under ",(0,t.yg)("inlineCode",{parentName:"li"},"actor_train"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"actor_infer"),", etc.:  ",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"model_args")," \u2013 Model architecture, dtype (e.g., bf16), attention type (e.g., flash_attn).  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"training_args")," \u2013 Learning rate, batch size, gradient accumulation.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"generating_args")," \u2013 ",(0,t.yg)("inlineCode",{parentName:"li"},"max_new_tokens"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"top_p"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"temperature")," for inference.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"strategy_args")," \u2013 Distributed strategy (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"deepspeed_train"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"vllm"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"sglang"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"hf_infer"),") and its specific configuration.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"device_mapping")," \u2013 GPU allocation for the worker.")))),(0,t.yg)("hr",null),(0,t.yg)("h3",{id:"environments-baseenv-and-implementations"},"Environments (",(0,t.yg)("inlineCode",{parentName:"h3"},"BaseEnv")," and implementations)"),(0,t.yg)("p",null,"ROLL supports various environments for agent training, typically located in subdirectories under ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/agentic/env/"),"."),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Base Class")," \u2013 ",(0,t.yg)("inlineCode",{parentName:"li"},"roll/agentic/env/base.py")," defines the interface for all environments, including methods like ",(0,t.yg)("inlineCode",{parentName:"li"},"reset")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"step"),".  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Examples")," \u2013 ",(0,t.yg)("inlineCode",{parentName:"li"},"FrozenLakeEnv"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"SokobanEnv"),". Specific configuration (e.g., map size, game rules) is defined within the ",(0,t.yg)("inlineCode",{parentName:"li"},"custom_envs")," section of the main YAML config.  "),(0,t.yg)("li",{parentName:"ul"},"Each environment type might also have its own specific config dataclass (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"roll/agentic/env/frozen_lake/config.py"),") that is instantiated based on the YAML.")),(0,t.yg)("hr",null),(0,t.yg)("h3",{id:"models--workers"},"Models & Workers"),(0,t.yg)("p",null,"The agentic pipeline typically involves multiple model roles, managed by dedicated workers:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Actor Model (",(0,t.yg)("inlineCode",{parentName:"strong"},"ActorWorker"),")")," \u2013 The policy that generates actions based on observations. It's trained to maximize expected rewards.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Critic Model (",(0,t.yg)("inlineCode",{parentName:"strong"},"CriticWorker"),")")," \u2013 Estimates the value function (e.g., expected return from a state). Used in algorithms like PPO with GAE to reduce variance and guide actor training.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Reference Model (",(0,t.yg)("inlineCode",{parentName:"strong"},"ActorWorker"),")")," \u2013 Often a snapshot of the initial policy or a fixed reference policy, used for calculating KL divergence to regularize policy updates.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Environment Manager (",(0,t.yg)("inlineCode",{parentName:"strong"},"EnvironmentWorker"),", managed by ",(0,t.yg)("inlineCode",{parentName:"strong"},"RolloutScheduler"),")")," \u2013 Handles stepping through multiple environment instances in parallel, collecting trajectories.")),(0,t.yg)("p",null,"Model paths, types, and distributed strategies are specified in their respective worker configuration blocks within the main YAML (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"actor_train"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"critic"),")."),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"running-the-agentic-pipeline"},"Running the Agentic Pipeline"),(0,t.yg)("h3",{id:"method1-using-the-python-launcher-script-with-hydra-recommended"},"Method","\xa0","1: Using the Python Launcher Script with Hydra (Recommended)"),(0,t.yg)("p",null,"The primary method is to use the ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/start_agentic_pipeline.py")," script. This script uses Hydra to load and manage configurations."),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Choose or Create a Configuration File"),(0,t.yg)("br",{parentName:"p"}),"\n","Start with an example YAML (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-0.5B-agentic_ds/agent_val_frozen_lake.yaml"),") or create your own.")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Execute the Python Launcher Script")),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Ensure you are in the root of the ROLL project directory\n# export PYTHONPATH=$(pwd):$PYTHONPATH\n\npython examples/start_agentic_pipeline.py \\\n       --config-path examples/qwen2.5-0.5B-agentic_ds \\\n       --config-name agent_val_frozen_lake\n")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"--config-path")," \u2013 Directory containing your YAML configuration.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"--config-name")," \u2013 Filename (without ",(0,t.yg)("inlineCode",{parentName:"li"},".yaml"),").  "),(0,t.yg)("li",{parentName:"ul"},"You can add Hydra overrides, e.g. ",(0,t.yg)("inlineCode",{parentName:"li"},"exp_name=my_new_experiment seed=123"),".")))),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"start_agentic_pipeline.py")," script:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Initializes Hydra.  "),(0,t.yg)("li",{parentName:"ul"},"Composes the configuration.  "),(0,t.yg)("li",{parentName:"ul"},"Converts the OmegaConf object into an ",(0,t.yg)("inlineCode",{parentName:"li"},"AgenticConfig")," dataclass instance.  "),(0,t.yg)("li",{parentName:"ul"},"Initializes Ray.  "),(0,t.yg)("li",{parentName:"ul"},"Instantiates the ",(0,t.yg)("inlineCode",{parentName:"li"},"AgenticPipeline")," and calls ",(0,t.yg)("inlineCode",{parentName:"li"},"run()"),".")),(0,t.yg)("hr",null),(0,t.yg)("h3",{id:"method2-using-helper-shell-scripts"},"Method","\xa0","2: Using Helper Shell Scripts"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"examples")," directory often contains shell scripts wrapping the Python launcher (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"run_agentic_pipeline_frozen_lake.sh"),")."),(0,t.yg)("p",null,"Example structure:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'#!/bin/bash\n# Example: examples/qwen2.5-0.5B-agentic_ds/run_agentic_pipeline_frozen_lake.sh\n\nCONFIG_NAME="agent_val_frozen_lake"                         # agent_val_frozen_lake.yaml\nCONFIG_PATH_DIR="examples/qwen2.5-0.5B-agentic_ds"\n\nSCRIPT_DIR=$(dirname "$(realpath "$0")")\nPROJECT_ROOT=$(dirname "$(dirname "$SCRIPT_DIR")")\n\nexport PYTHONPATH=$PROJECT_ROOT:$PYTHONPATH\nexport RAY_LOG_WITHOUT_ANSI_CODES=1\n\necho "PYTHONPATH: $PYTHONPATH"\necho "Using Hydra config directory: $CONFIG_PATH_DIR and config name: $CONFIG_NAME"\n\npython $PROJECT_ROOT/examples/start_agentic_pipeline.py \\\n       --config-path $CONFIG_PATH_DIR \\\n       --config-name $CONFIG_NAME \\\n       "$@"   # Pass through any extra args\n')),(0,t.yg)("p",null,"Run it with:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"bash examples/qwen2.5-0.5B-agentic_ds/run_agentic_pipeline_frozen_lake.sh\n")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"step-by-step-example"},"Step-by-Step Example"),(0,t.yg)("h3",{id:"step1-locate-and-review-configuration"},"Step","\xa0","1: Locate and Review Configuration"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"File: ",(0,t.yg)("inlineCode",{parentName:"li"},"examples/qwen2.5-0.5B-agentic_ds/agent_val_frozen_lake.yaml"),(0,t.yg)("br",{parentName:"li"}),"Key sections include ",(0,t.yg)("inlineCode",{parentName:"li"},"exp_name"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"seed"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"output_dir"),", model paths, ",(0,t.yg)("inlineCode",{parentName:"li"},"actor_train"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"actor_infer"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"reference"),", PPO parameters, and ",(0,t.yg)("inlineCode",{parentName:"li"},"custom_envs"),".")),(0,t.yg)("h3",{id:"step2-prepare-the-environment-and-dependencies"},"Step","\xa0","2: Prepare the Environment and Dependencies"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Ensure ",(0,t.yg)("inlineCode",{parentName:"p"},"gymnasium")," (or the relevant env dependency) is installed:"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install gymnasium\n"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Verify all model paths in the YAML are accessible."))),(0,t.yg)("h3",{id:"step3-launch-the-pipeline"},"Step","\xa0","3: Launch the Pipeline"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"python examples/start_agentic_pipeline.py \\\n       --config-path examples/qwen2.5-0.5B-agentic_ds \\\n       --config-name agent_val_frozen_lake\n")),(0,t.yg)("h3",{id:"step4-monitoring"},"Step","\xa0","4: Monitoring"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Console Output")," \u2013 Observe Hydra, Ray, and pipeline logs.  ")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Log Files")," \u2013 Check the ",(0,t.yg)("inlineCode",{parentName:"p"},"logging_dir")," specified in YAML.  ")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Experiment Tracking")," \u2013 If configured, metrics appear in WandB.  ")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"TensorBoard")),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"tensorboard --logdir <your_log_dir>\n")))),(0,t.yg)("h3",{id:"step5-outputs-and-results"},"Step","\xa0","5: Outputs and Results"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Trained Models")," \u2013 Checkpoints saved under ",(0,t.yg)("inlineCode",{parentName:"li"},"output_dir"),".  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Evaluation Metrics")," \u2013 Logged in WandB/TensorBoard and console.")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Import Errors")," \u2013 Ensure ",(0,t.yg)("inlineCode",{parentName:"li"},"PYTHONPATH")," includes the ROLL project root.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Configuration Errors"),"  ",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"YAML syntax \u2013 lint your YAML.  "),(0,t.yg)("li",{parentName:"ul"},"Hydra path/name \u2013 verify ",(0,t.yg)("inlineCode",{parentName:"li"},"--config-path")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"--config-name"),".  "),(0,t.yg)("li",{parentName:"ul"},"Pydantic validation \u2013 check ",(0,t.yg)("inlineCode",{parentName:"li"},"roll/pipeline/agentic/agentic_config.py")," field definitions.  "))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Model Loading Issues")," \u2013 Confirm paths, model types, and strategies (vLLM, SGLang, DeepSpeed, Megatron-Core).  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"CUDA/GPU Issues")," \u2013 Adjust ",(0,t.yg)("inlineCode",{parentName:"li"},"CUDA_VISIBLE_DEVICES"),", batch sizes, or ",(0,t.yg)("inlineCode",{parentName:"li"},"gpu_memory_utilization"),".  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Ray Issues")," \u2013 Ensure Ray is started and resource requests match hardware.  "),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Environment Registration")," \u2013 Verify ",(0,t.yg)("inlineCode",{parentName:"li"},"env_type")," values correspond to registered env classes.")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"further-information"},"Further Information"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Full configuration definitions: ",(0,t.yg)("inlineCode",{parentName:"li"},"roll/pipeline/agentic/agentic_config.py")," and related dataclasses.  "),(0,t.yg)("li",{parentName:"ul"},"Environment implementations: ",(0,t.yg)("inlineCode",{parentName:"li"},"roll/agentic/env/")," (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"frozen_lake/env.py"),").  "),(0,t.yg)("li",{parentName:"ul"},"Core pipeline logic: ",(0,t.yg)("inlineCode",{parentName:"li"},"roll/pipeline/agentic/agentic_pipeline.py"),".  "),(0,t.yg)("li",{parentName:"ul"},"The project ",(0,t.yg)("inlineCode",{parentName:"li"},"README.md")," provides additional high-level details on features like GRPO, Reasoning Pipeline, and integrations with Ray, DeepSpeed, Megatron-Core, vLLM, and SGlang.")),(0,t.yg)("hr",null),(0,t.yg)("p",null,(0,t.yg)("em",{parentName:"p"},"Happy experimenting!")))}y.isMDXComponent=!0},5680:(e,n,i)=>{i.d(n,{xA:()=>s,yg:()=>c});var a=i(6540);function t(e,n,i){return n in e?Object.defineProperty(e,n,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[n]=i,e}function r(e,n){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),i.push.apply(i,a)}return i}function l(e){for(var n=1;n<arguments.length;n++){var i=null!=arguments[n]?arguments[n]:{};n%2?r(Object(i),!0).forEach(function(n){t(e,n,i[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(i,n))})}return e}function o(e,n){if(null==e)return{};var i,a,t=function(e,n){if(null==e)return{};var i,a,t={},r=Object.keys(e);for(a=0;a<r.length;a++)i=r[a],n.indexOf(i)>=0||(t[i]=e[i]);return t}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)i=r[a],n.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(t[i]=e[i])}return t}var p=a.createContext({}),g=function(e){var n=a.useContext(p),i=n;return e&&(i="function"==typeof e?e(n):l(l({},n),e)),i},s=function(e){var n=g(e.components);return a.createElement(p.Provider,{value:n},e.children)},m="mdxType",y={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef(function(e,n){var i=e.components,t=e.mdxType,r=e.originalType,p=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),m=g(i),d=t,c=m["".concat(p,".").concat(d)]||m[d]||y[d]||r;return i?a.createElement(c,l(l({ref:n},s),{},{components:i})):a.createElement(c,l({ref:n},s))});function c(e,n){var i=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var r=i.length,l=new Array(r);l[0]=d;var o={};for(var p in n)hasOwnProperty.call(n,p)&&(o[p]=n[p]);o.originalType=e,o[m]="string"==typeof e?e:t,l[1]=o;for(var g=2;g<r;g++)l[g]=i[g];return a.createElement.apply(null,l)}return a.createElement.apply(null,i)}d.displayName="MDXCreateElement"}}]);